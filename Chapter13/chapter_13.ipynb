{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/), [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/) and [Matthieu Brucher](https://www.linkedin.com/in/matthieubrucher/) published by PACKT Publishing.\n",
    "\n",
    "It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHART_DIR = \"charts\"\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)\n",
    "\n",
    "def save_png(name):\n",
    "    fn = 'B09124_13_%s.png'%name # please ignore, it just helps our publisher :-)\n",
    "    plt.savefig(os.path.join(CHART_DIR, fn), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Q function the old fashion way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a table with some Q values for this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an empty table\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "# Set learning hyperparameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "\n",
    "# Let's run!\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment and get first new observation (top left)\n",
    "    s = env.reset()\n",
    "    # Do 100 iterations to update the table\n",
    "    for i in range(100):\n",
    "        # Choose an action by picking the max of the table + additional random noise ponderated by the episode\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)//(i+1))\n",
    "        # Get new state and reward from environment after chosen step \n",
    "        s1, r, d,_ = env.step(a)\n",
    "        # Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test games with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0.99\n",
    "e = 0.1 # 1 in 10 samples, we chose a new action for the network\n",
    "num_episodes = 2000\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# A simple one layer network\n",
    "inputs = tf.placeholder(shape=[None, 16], dtype=tf.float32, name=\"input\")\n",
    "Qout = tf.layers.dense(\n",
    "    inputs=inputs,\n",
    "    units=4,\n",
    "    use_bias=False,\n",
    "    name=\"dense\",\n",
    "    kernel_initializer=tf.random_uniform_initializer(minval=0, maxval=.0125)\n",
    ")\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "# Our optimizer will try to optimize \n",
    "nextQ = tf.placeholder(shape=[None, 4], dtype=tf.float32, name=\"target\")\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the network, and check that it will get more and more sucesses as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep track of our games and our results\n",
    "jList = []\n",
    "rList = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        \n",
    "        for j in range(100):\n",
    "            a, targetQ = sess.run([predict, Qout], feed_dict={inputs:np.identity(16)[s:s+1]})\n",
    "            # We randomly choose a new state that we may have not encountered before\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            \n",
    "            # Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs:np.identity(16)[s1:s1+1]})\n",
    "            # Obtain maxQ' and set our target value for chosen action.\n",
    "            targetQ[0, a[0]] = r + y*np.max(Q1)\n",
    "            \n",
    "            # Train our network using target and predicted Q values\n",
    "            sess.run(updateModel, feed_dict={inputs:np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # Reduce chance of random action as we train the model.\n",
    "                e = 1 / ((i // 50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print(\"Percent of succesful episodes: %f%%\" % (sum(rList) / num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the evolution of the reward with each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "\n",
    "plt.plot(lfilter(np.ones(20)/20, [1], rList))\n",
    "save_png(\"reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the survival increases, even if we take suoptimal paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jList)\n",
    "save_png(\"length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here was inspired by several tutorials and courses online:\n",
    "* https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
    "* https://github.com/tokb23/dqn\n",
    "* https://github.com/dennybritz/reinforcement-learning/blob/master/DQN/dqn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now design a a network that can tackle more or less any of the Atari games available on the gym plaform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import os\n",
    "import six\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import itertools\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "CHART_DIR = \"charts\"\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few helper function, one to preprocess our images and shrink them and two others that will transpose the data. The reason is that we use the past images as additional channels, so the axis order is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))[None,:,:]\n",
    "\n",
    "def adapt_state(state):\n",
    "    return [np.float32(np.transpose(state, (2, 1, 0)) / 255.0)]\n",
    "\n",
    "def adapt_batch_state(state):\n",
    "    return np.transpose(np.array(state), (0, 3, 2, 1)) / 255.0\n",
    "\n",
    "def get_initial_state(frame):\n",
    "    processed_frame = preprocess(frame)\n",
    "    state = [processed_frame for _ in range(state_length)]\n",
    "    return np.concatenate(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a bunch of hyperparameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_name = \"Breakout-v4\"\n",
    "\n",
    "width = 80  # Resized frame width\n",
    "height = 105  # Resized frame height\n",
    "\n",
    "n_episodes = 12000  # Number of runs for the agent\n",
    "state_length = 4  # Number of most frames we input to the network\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "exploration_steps = 1000000  # During all these steps, we progressively lower epsilon\n",
    "initial_epsilon = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "final_epsilon = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "\n",
    "initial_random_search = 20000  # Number of steps to populate the replay memory before training starts\n",
    "replay_memory_size = 400000  # Number of states we keep for training\n",
    "batch_size = 32  # Batch size\n",
    "network_update_interval = 10000  # The frequency with which the target network is updated\n",
    "train_skips = 4  # The agent selects 4 actions between successive updates\n",
    "\n",
    "learning_rate = 0.00025  # Learning rate used by RMSProp\n",
    "momentum = 0.95  # momentum used by RMSProp\n",
    "min_gradient = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "\n",
    "network_path = 'saved_networks/' + env_name\n",
    "tensorboard_path = 'summary/' + env_name\n",
    "save_interval = 300000  # The frequency with which the network is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a class to train, save and restore our network. We will use one instance for the Q network and another one for the target network.\n",
    "get_trained_action() will be the method used to get a new action from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_step = (initial_epsilon - final_epsilon) / exploration_steps\n",
    "        \n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self.build_model()\n",
    "        if summaries_dir:\n",
    "            summary_dir = os.path.join(summaries_dir, \"summaries_%s\" % scope)\n",
    "            if not os.path.exists(summary_dir):\n",
    "                os.makedirs(summary_dir)\n",
    "            self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "        self.X = tf.placeholder(shape=[None, width, height, state_length], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        model = tf.keras.Sequential(self.scope)\n",
    "        model.add(tf.keras.layers.Convolution2D(filters=32, kernel_size=8, strides=(4, 4), activation='relu', input_shape=(width, height, state_length), name=\"Layer1\"))\n",
    "        model.add(tf.keras.layers.Convolution2D(filters=64, kernel_size=4, strides=(2, 2), activation='relu', name=\"Layer2\"))\n",
    "        model.add(tf.keras.layers.Convolution2D(filters=64, kernel_size=3, strides=(1, 1), activation='relu', name=\"Layer3\"))\n",
    "        model.add(tf.keras.layers.Flatten(name=\"Flatten\"))\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu', name=\"Layer4\"))\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, name=\"Output\"))\n",
    "\n",
    "        self.predictions = model(self.X)\n",
    "\n",
    "        a_one_hot = tf.one_hot(self.actions, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.predictions, a_one_hot), reduction_indices=1)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y, q_value)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate, momentum=momentum, epsilon=min_gradient)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        return sess.run(self.predictions, { self.X: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        feed_dict = { self.X: s, self.y: y, self.actions: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.train.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, sess, state):\n",
    "        if self.epsilon >= random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.predict(sess, adapt_state(state)))\n",
    "\n",
    "        # Decay epsilon over time\n",
    "        if self.epsilon > final_epsilon:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_trained_action(self, state):\n",
    "        action = np.argmax(self.predict(sess, adapt_state(state)))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create also a function to copy parameters from one network to the other, a function to create an initial clean state as well as a function to create the summary reports for scalar by episode outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "    Args:\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    return update_ops\n",
    "\n",
    "def create_memory(env):\n",
    "    # Populate the replay memory with initial experience    \n",
    "    replay_memory = []\n",
    "    \n",
    "    frame = env.reset()\n",
    "    state = get_initial_state(frame)\n",
    "\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = np.random.choice(np.arange(env.action_space.n))\n",
    "        frame, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_state = np.append(state[1:, :, :], preprocess(frame), axis=0)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            frame = env.reset()\n",
    "            state = get_initial_state(frame)\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    return replay_memory\n",
    "\n",
    "\n",
    "def setup_summary():\n",
    "    with tf.variable_scope(\"episode\"):\n",
    "        episode_total_reward = tf.Variable(0., name=\"EpisodeTotalReward\")\n",
    "        tf.summary.scalar('Total Reward', episode_total_reward)\n",
    "        episode_avg_max_q = tf.Variable(0., name=\"EpisodeAvgMaxQ\")\n",
    "        tf.summary.scalar('Average Max Q', episode_avg_max_q)\n",
    "        episode_duration = tf.Variable(0., name=\"EpisodeDuration\")\n",
    "        tf.summary.scalar('Duration', episode_duration)\n",
    "        episode_avg_loss = tf.Variable(0., name=\"EpisodeAverageLoss\")\n",
    "        tf.summary.scalar('Average Loss', episode_avg_loss)\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q, episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.summary.merge_all(scope=\"episode\")\n",
    "    return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our network (and save some final images from the trained network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "env = gym.make(env_name)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create estimators\n",
    "q_estimator = Estimator(env, scope=\"q\", summaries_dir=tensorboard_path)\n",
    "target_estimator = Estimator(env, scope=\"target_q\")\n",
    "\n",
    "copy_model = copy_model_parameters(q_estimator, target_estimator)\n",
    "\n",
    "summary_placeholders, update_ops, summary_op = setup_summary()\n",
    "\n",
    "# The replay memory\n",
    "replay_memory = create_memory(env)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    q_estimator.summary_writer.add_graph(sess.graph)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(network_path)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint %s...\\n\" % latest_checkpoint)\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        if total_t % save_interval == 0:\n",
    "            # Save the current checkpoint\n",
    "            saver.save(tf.get_default_session(), network_path)\n",
    "\n",
    "        frame = env.reset()\n",
    "        state = get_initial_state(frame)\n",
    "\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        total_q_max = 0\n",
    "\n",
    "        for duration in itertools.count():    \n",
    "            # Maybe update the target estimator\n",
    "            if total_t % network_update_interval == 0:\n",
    "                sess.run(copy_model)\n",
    "\n",
    "            action = q_estimator.get_action(sess, state)\n",
    "            frame, reward, terminal, _ = env.step(action)\n",
    "\n",
    "            processed_frame = preprocess(frame)\n",
    "            next_state = np.append(state[1:, :, :], processed_frame, axis=0)\n",
    "\n",
    "            reward = np.clip(reward, -1, 1)\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, terminal))\n",
    "            if len(replay_memory) > replay_memory_size:\n",
    "                replay_memory.popleft()\n",
    "\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets (Double DQN)\n",
    "            adapted_state = adapt_batch_state(next_states_batch)\n",
    "\n",
    "            q_values_next = q_estimator.predict(sess, adapted_state)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = target_estimator.predict(sess, adapted_state)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * gamma * q_values_next_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = adapt_batch_state(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            total_q_max += np.max(q_values_next)\n",
    "            total_loss += loss\n",
    "            total_t += 1\n",
    "            total_reward += reward\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        stats = [total_reward, total_q_max / duration, duration, total_loss / duration]\n",
    "        for i in range(len(stats)):\n",
    "            sess.run(update_ops[i], feed_dict={\n",
    "                summary_placeholders[i]: float(stats[i])\n",
    "            })\n",
    "        summary_str = sess.run(summary_op, )\n",
    "        q_estimator.summary_writer.add_summary(summary_str, episode)\n",
    "\n",
    "        env.env.ale.saveScreenPNG(six.b('%s/test_image_%05i.png' % (CHART_DIR, episode)))\n",
    "\n",
    "    # Save the last checkpoint\n",
    "    saver.save(tf.get_default_session(), network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

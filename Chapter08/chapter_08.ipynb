{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on MNIST with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book Building Machine Learning Systems with Python by Willi Richert, Luis Pedro Coelho and Matthieu Brucher published by PACKT Publishing\n",
    "\n",
    "It is made available under the MIT License\n",
    "\n",
    "Let's try to classify the MNIST database (written digits) with a convolutional network.\n",
    "\n",
    "We will start with some hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_epochs = 10\n",
    "learning_rate = 0.0002\n",
    "batch_size = 128\n",
    "image_shape = [28,28,1]\n",
    "step = 1000\n",
    "export_dir = \"data/classifier-mnist\"\n",
    "dim_W1 = 1024\n",
    "dim_W2 = 128\n",
    "dim_W3 = 64\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to load the data and shape it as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist.data.shape = (-1, 28, 28)\n",
    "mnist.data = mnist.data.astype(np.float32).reshape( [-1, 28, 28, 1]) / 255.\n",
    "mnist.num_examples = len(mnist.data)\n",
    "mnist.labels = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should split our data between training and testing data (6 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.labels, test_size=(1. / 7.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional network builder will be stored in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_shape=[28,28,1],\n",
    "            dim_W1=1024,\n",
    "            dim_W2=128,\n",
    "            dim_W3=64,\n",
    "            classes=10\n",
    "            ):\n",
    "\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "        self.dim_W1 = dim_W1\n",
    "        self.dim_W2 = dim_W2\n",
    "        self.dim_W3 = dim_W3\n",
    "        self.classes = classes\n",
    "\n",
    "    def build_model(self):\n",
    "        image = tf.placeholder(tf.float32, [None]+self.image_shape, name=\"image\")\n",
    "        Y = tf.placeholder(tf.int64, [None], name=\"label\")\n",
    "        training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        probabilities = self.discriminate(image, training)\n",
    "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=probabilities))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(probabilities, axis=1), Y), tf.float32), name=\"accuracy\")\n",
    "\n",
    "        return image, Y, cost, accuracy, probabilities, training\n",
    "\n",
    "    def create_conv2d(self, input, filters, kernel_size, name):\n",
    "        layer = tf.layers.conv2d(\n",
    "                    inputs=input,\n",
    "                    filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    name=\"Conv2d_\" + name,\n",
    "                    padding=\"same\")\n",
    "        return layer\n",
    "    \n",
    "    def create_maxpool(self, input, name):\n",
    "        layer = tf.layers.max_pooling2d(\n",
    "                    inputs=input,\n",
    "                    pool_size=[2,2],\n",
    "                    strides=2,\n",
    "                    name=\"MaxPool_\" + name)\n",
    "        return layer\n",
    "\n",
    "    def create_dropout(self, input, name, is_training):\n",
    "        layer = tf.layers.dropout(\n",
    "                    inputs=input,\n",
    "                    rate=dropout_rate,\n",
    "                    name=\"DropOut_\" + name,\n",
    "                    training=is_training)\n",
    "        return layer\n",
    "\n",
    "    def create_dense(self, input, units, name):\n",
    "        layer = tf.layers.dense(\n",
    "                inputs=input,\n",
    "                units=units,\n",
    "                name=\"Dense\" + name,\n",
    "                )\n",
    "        layer = tf.layers.batch_normalization(\n",
    "                inputs=layer,\n",
    "                momentum=0,\n",
    "                epsilon=1e-8,\n",
    "                training=True,\n",
    "                name=\"BatchNorm_\" + name,\n",
    "        )\n",
    "        layer = tf.nn.leaky_relu(layer, name=\"LeakyRELU_\" + name)\n",
    "        return layer\n",
    "\n",
    "    def discriminate(self, image, training):\n",
    "        h1 = self.create_conv2d(image, self.dim_W3, 5, \"Layer1\")\n",
    "        h1 = self.create_maxpool(h1, \"Layer1\")\n",
    "\n",
    "        h2 = self.create_conv2d(h1, self.dim_W2, 5, \"Layer2\")\n",
    "        h2 = self.create_maxpool(h2, \"Layer2\")\n",
    "        h2 = tf.reshape(h2, (-1, self.dim_W2 * 7 * 7))\n",
    "\n",
    "        h3 = self.create_dense(h2, self.dim_W1, \"Layer3\")\n",
    "        h3 = self.create_dropout(h3, \"Layer3\", training)\n",
    "        \n",
    "        h4 = self.create_dense(h3, self.classes, \"Layer4\")\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can instantiate it and create our optimizer. We take the opportunity to create our two objects to save the Tensorflow graph, Saver and builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "cnn_model = CNN(\n",
    "        image_shape=image_shape,\n",
    "        dim_W1=dim_W1,\n",
    "        dim_W2=dim_W2,\n",
    "        dim_W3=dim_W3,\n",
    "        )\n",
    "image_tf, Y_tf, cost_tf, accuracy_tf, output_tf, training_tf = cnn_model.build_model()\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(cost_tf)\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function that computes the global loss for the training and the testing data.\n",
    "It will be used for each epoch, but in real life, you should \"trust\" the partial loss instead, as this value is very costly to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vec = []\n",
    "\n",
    "def show_train(sess, epoch):\n",
    "    traccuracy = []\n",
    "    teaccuracy = []\n",
    "    for j in range(0, len(X_train), batch_size):\n",
    "        Xs = X_train[j:j+batch_size]\n",
    "        Ys = y_train[j:j+batch_size]\n",
    "        traccuracy.append(sess.run(accuracy_tf,\n",
    "                feed_dict={\n",
    "                    training_tf: False,\n",
    "                    Y_tf: Ys,\n",
    "                    image_tf: Xs\n",
    "                    }))\n",
    "    for j in range(0, len(X_test), batch_size):\n",
    "        Xs = X_test[j:j+batch_size]\n",
    "        Ys = y_test[j:j+batch_size]\n",
    "        teaccuracy.append(sess.run(accuracy_tf,\n",
    "                feed_dict={\n",
    "                    training_tf: False,\n",
    "                    Y_tf: Ys,\n",
    "                    image_tf: Xs,\n",
    "                    }))\n",
    "    train_accuracy = np.mean(traccuracy)\n",
    "    test_accuracy = np.mean(teaccuracy)\n",
    "    accuracy_vec.append((train_accuracy, test_accuracy))\n",
    "    \n",
    "    result = sess.run(output_tf,\n",
    "                feed_dict={\n",
    "                    training_tf: False,\n",
    "                    image_tf: X_test[:10]\n",
    "                    })\n",
    "    \n",
    "    print('Epoch #%i\\n  train accuracy = %f\\n  test accuracy = %f' % (epoch, train_accuracy, test_accuracy))\n",
    "    print('Result for the 10 first training images: %s' % np.argmax(result, axis=1))\n",
    "    print('Reference for the 10 first training images: %s' % y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    show_train(sess, -1)\n",
    "    for epoch in range(n_epochs):\n",
    "        permut = np.random.permutation(len(X_train))\n",
    "\n",
    "        print(\"epoch: %i\" % epoch)\n",
    "        for j in range(0, len(X_train), batch_size):\n",
    "            if j % step == 0:\n",
    "                print(\"  batch: %i\" % j)\n",
    "\n",
    "            batch = permut[j:j+batch_size]\n",
    "            Xs = X_train[batch]\n",
    "            Ys = y_train[batch]\n",
    "\n",
    "            sess.run(train_step,\n",
    "                    feed_dict={\n",
    "                        training_tf: True,\n",
    "                        Y_tf: Ys,\n",
    "                        image_tf: Xs\n",
    "                        })\n",
    "            if j % step == 0:\n",
    "                temp_cost, temp_prec = sess.run([cost_tf, accuracy_tf],\n",
    "                    feed_dict={\n",
    "                        training_tf: False,\n",
    "                        Y_tf: Ys,\n",
    "                        image_tf: Xs\n",
    "                        })\n",
    "                print(\"    cost: %f\\n    prec: %f\" % (temp_cost, temp_prec))\n",
    "        saver.save(sess, './classifier', global_step=epoch)\n",
    "        show_train(sess, epoch)\n",
    "    saver.save(sess, './classifier-final')\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                       [tf.saved_model.tag_constants.TRAINING])\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the global training and testing cost, as we created a function to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "accuracy = np.array(accuracy_vec)\n",
    "plt.semilogy(1 - accuracy[:,0], 'k-', label=\"train\")\n",
    "plt.semilogy(1 - accuracy[:,1], 'r-', label=\"test\")\n",
    "plt.title('Classification error per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check that Saver allowed to properly save and restore the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "new_saver = tf.train.import_meta_graph(\"classifier-final.meta\")  \n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    training_tf = graph.get_tensor_by_name('is_training:0')\n",
    "    Y_tf = graph.get_tensor_by_name('label:0')\n",
    "    image_tf = graph.get_tensor_by_name('image:0')\n",
    "    accuracy_tf = graph.get_tensor_by_name('accuracy:0')\n",
    "    output_tf = graph.get_tensor_by_name('LeakyRELU_Layer4/Maximum:0')\n",
    "    \n",
    "    show_train(sess, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:  \n",
    "    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.TRAINING], export_dir)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    training_tf = graph.get_tensor_by_name('is_training:0')\n",
    "    Y_tf = graph.get_tensor_by_name('label:0')\n",
    "    image_tf = graph.get_tensor_by_name('image:0')\n",
    "    accuracy_tf = graph.get_tensor_by_name('accuracy:0')\n",
    "    output_tf = graph.get_tensor_by_name('LeakyRELU_Layer4/Maximum:0')\n",
    "\n",
    "    show_train(sess, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test prediction with LTSMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs are good tools to predict new values in a sequence. Can they predict text from Aesop's fables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"A slave named Androcles once escaped from his master and fled to the forest. As he was wandering about there he came upon a Lion lying down moaning and groaning. At first he turned to flee, but finding that the Lion did not pursue him, he turned back and went up to him.\n",
    "As he came near, the Lion put out his paw, which was all swollen and bleeding, and Androcles found that a huge thorn had got into it, and was causing all the pain. He pulled out the thorn and bound up the paw of the Lion, who was soon able to rise and lick the hand of Androcles like a dog. Then the Lion took Androcles to his cave, and every day used to bring him meat from which to live.\n",
    "But shortly afterwards both Androcles and the Lion were captured, and the slave was sentenced to be thrown to the Lion, after the latter had been kept without food for several days. The Emperor and all his Court came to see the spectacle, and Androcles was led out into the middle of the arena. Soon the Lion was let loose from his den, and rushed bounding and roaring towards his victim.\n",
    "But as soon as he came near to Androcles he recognised his friend, and fawned upon him, and licked his hands like a friendly dog. The Emperor, surprised at this, summoned Androcles to him, who told him the whole story. Whereupon the slave was pardoned and freed, and the Lion let loose to his native forest.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know remove commas and points and then split the text by words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = text.lower().replace(\",\", \"\").replace(\".\", \"\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python itsef has a module to count words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "training_data_args = [dictionary[word] for word in training_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RNN will be a simple LSTM layer and then a dense layer to specify the word it selected. The input will be split so that we get several elements each time (here 3 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "def RNN(x):\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but we only want the last output\n",
    "    return tf.layers.dense(inputs = outputs[-1], units = vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add our traditional hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "# number of inputs (past words that we use)\n",
    "n_input = 3\n",
    "# number of units in the RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int64, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the functions to optimize and our prediction functions as well. As for the MNIST CNN, we use sparse_softmax_cross_entropy_with_logits because we only want one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RNN(x)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This train loop is a little bit different than the previous ones, as it does one sample at a time, and then we average the loss and the accuracy before we display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    while step < training_iters:\n",
    "        # Batch with just one sample. Add some randomness on selection process.\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ [training_data_args[i]] for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [1, n_input])\n",
    "\n",
    "        symbols_out_onehot = [training_data_args[offset+n_input]]\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= %i , Average Loss= %.6f, Average Accuracy= %.2f%%\" % (step+1, loss_total/display_step, 100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = training_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[np.argmax(onehot_pred, axis=1)[0]]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in, symbols_out, symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start this time with hyperparameters because the way we reshape our images depends on our network archtecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#rows of 28 pixels\n",
    "n_input=28\n",
    "#unrolled through 28 time steps (our images are (28,28))\n",
    "time_steps=28\n",
    "\n",
    "#hidden LSTM units\n",
    "num_units=128\n",
    "\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "n_classes=10\n",
    "batch_size=128\n",
    "\n",
    "n_epochs = 10\n",
    "step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist.data = mnist.data.astype(np.float32).reshape( [-1, time_steps, n_input]) / 255.\n",
    "mnist.num_examples = len(mnist.data)\n",
    "mnist.labels = mnist.target.astype(np.int8)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.labels, test_size=(1. / 7.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the network we will use (we don't store it in a class this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,time_steps,n_input])\n",
    "y=tf.placeholder(tf.int64,[None])\n",
    "\n",
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(x ,time_steps,1)\n",
    "\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=True)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=tf.float32)\n",
    "\n",
    "prediction=tf.layers.dense(inputs=outputs[-1], units = n_classes)\n",
    "\n",
    "loss=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),y)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        permut = np.random.permutation(len(X_train))\n",
    "        print(\"epoch: %i\" % epoch)\n",
    "        for j in range(0, len(X_train), batch_size):\n",
    "            if j % step == 0:\n",
    "                print(\"  batch: %i\" % j)\n",
    "\n",
    "            batch = permut[j:j+batch_size]\n",
    "            Xs = X_train[batch]\n",
    "            Ys = y_train[batch]\n",
    "\n",
    "            sess.run(opt, feed_dict={x: Xs, y: Ys})\n",
    "\n",
    "            if j % step == 0:\n",
    "                acc=sess.run(accuracy,feed_dict={x:Xs,y:Ys})\n",
    "                los=sess.run(loss,feed_dict={x:Xs,y:Ys})\n",
    "                print(\"  accuracy %f\" % acc)\n",
    "                print(\"  loss %f\" % los)\n",
    "                print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

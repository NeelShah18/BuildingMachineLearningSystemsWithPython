{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Systems with Python - Chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will create a music genre classifier. While the ML algorithm itself (logistic regression) is nothing fancy by now, we will look into fancy features like Fast Fourier Transforms and Mel Frequency Cepstral Coefficients, use P/R and ROC curves to analyze what works best and then figure out which version to use.\n",
    "\n",
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/), [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/) and [Matthieu Brucher](https://www.linkedin.com/in/matthieubrucher/) published by PACKT Publishing. It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "from pathlib import Path\n",
    "\n",
    "CHART_DIR = \"charts\"\n",
    "if not Path(CHART_DIR).exists():\n",
    "    os.mkdir(CHART_DIR)\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "if not Path(DATA_DIR).exists():\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "GENRE_DIR = Path(DATA_DIR) / 'genres'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "DPI = 100\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "def save_png(name):\n",
    "    fn = 'B09124_11_%s.png'%name # please ignore, it just helps our publisher :-)\n",
    "    plt.savefig(str(Path(CHART_DIR) / fn), bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plot_pr(auc_score, name, precision, recall, label=None, plot_nr=None):\n",
    "    plt.figure(num=None, figsize=(5, 4), dpi=DPI)\n",
    "    plt.grid(True)\n",
    "    plt.fill_between(recall, precision, alpha=0.5)\n",
    "    plt.plot(recall, precision, lw=1)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('P/R curve (AUC=%0.2f) / %s' % (auc_score, label))\n",
    "    filename = name.replace(\" \", \"_\")\n",
    "    save_png(\"%s_pr_%s\" % (plot_nr, filename))\n",
    "    \n",
    "def plot_roc(auc_score, name, tpr, fpr, label=None, plot_nr=None):\n",
    "    plt.figure(num=None, figsize=(5, 4), dpi=DPI)\n",
    "    plt.grid(True)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.fill_between(fpr, tpr, alpha=0.5)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve (AUC = %0.2f) / %s' % (auc_score, label), verticalalignment=\"bottom\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    save_png('%i_auc_%s' % (plot_nr, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "genre_fn = 'http://opihi.cs.uvic.ca/sound/genres.tar.gz'\n",
    "urllib.request.urlretrieve(genre_fn, Path(DATA_DIR) / 'genres.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "try:\n",
    "    f = tarfile.open('genres.tar.gz', 'r:gz')\n",
    "    try: \n",
    "        f.extractall()\n",
    "    finally: \n",
    "        f.close()\n",
    "finally:\n",
    "    os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and install https://sourceforge.net/projects/sox/files/sox/. For this notebook, we are using SOX 14.4.2 to convert the downloaded genre files from `.au` into `.wav` format, which is easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOX_PATH = r'C:\\Program Files (x86)\\sox-14-4-2'\n",
    "SOX = SOX_PATH + r'\\sox.exe'\n",
    "SOX = \"sox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for au_fn in Path(GENRE_DIR).glob('**/*.au'):\n",
    "    print(au_fn)\n",
    "    !\"{SOX}\" {au_fn} {au_fn.with_suffix('.wav')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "\n",
    "from matplotlib.ticker import EngFormatter\n",
    "\n",
    "def plot_specgram(ax, fn):\n",
    "    sample_rate, X = scipy.io.wavfile.read(fn)\n",
    "    ax.specgram(X, Fs=sample_rate, xextent=(0, 30), cmap='hot')\n",
    "\n",
    "GENRES = [\"classical\", \"jazz\", \"country\", \"pop\", \"rock\", \"metal\"]\n",
    "\n",
    "def plot_specgrams():\n",
    "    \"\"\"\n",
    "    Plot a bunch of spectrograms of wav files in different genres\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    \n",
    "    num_files = 3\n",
    "    f, axes = plt.subplots(len(GENRES), num_files, dpi=DPI, figsize=(6, 8))\n",
    "    \n",
    "    for genre_idx, genre in enumerate(GENRES):\n",
    "        for idx, fn in enumerate((Path(GENRE_DIR) / genre).glob('*.wav')):\n",
    "            if idx == num_files:\n",
    "                break\n",
    "            \n",
    "            axis = axes[genre_idx, idx]\n",
    "            axis.tick_params(direction='out', length=0, width=1, labelsize=5)\n",
    "    \n",
    "            axis.yaxis.set_major_formatter(EngFormatter())\n",
    "            axis.set_title(\"%s song %i\" % (genre, idx + 1), fontsize=7)\n",
    "            plot_specgram(axis, fn)\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    save_png(\"5_Spectrogram_Genres\")\n",
    "    \n",
    "plot_specgrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!\"{SOX}\" --null -r 22050 sine_a.wav synth 0.2 sine 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!\"{SOX}\" --null -r 22050 sine_b.wav synth 0.2 sine 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!\"{SOX}\" --combine mix --volume 1 sine_b.wav --volume 0.5 sine_a.wav sine_mix.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three files `sine_a.wav`, `sine_b.wav`, `sine_mix.wav` in the current directory, which we can visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def plot_wav_fft(wav_filename, desc=None, plot=0):\n",
    "    plt.clf()\n",
    "    plt.figure(num=None, figsize=(6, 4), dpi=DPI)\n",
    "    sample_rate, X = scipy.io.wavfile.read(wav_filename)\n",
    "    spectrum = np.fft.fft(X)\n",
    "    freq = np.fft.fftfreq(len(X), 1.0 / sample_rate)\n",
    "\n",
    "    plt.subplot(211)\n",
    "    num_samples = 200\n",
    "    plt.xlim(0, num_samples / sample_rate)\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    plt.title(desc or wav_filename)\n",
    "    plt.plot(np.arange(num_samples) / sample_rate, X[:num_samples])\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.xlim(0, 5000)\n",
    "    plt.xlabel(\"frequency [Hz]\")\n",
    "    plt.xticks(np.arange(5) * 1000)\n",
    "    if desc:\n",
    "        desc = desc.strip()\n",
    "        fft_desc = desc[0].lower() + desc[1:]\n",
    "    else:\n",
    "        fft_desc = wav_filename\n",
    "    plt.title(\"FFT of %s\" % fft_desc)\n",
    "    plt.plot(freq, abs(spectrum), linewidth=2)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    rel_filename = os.path.split(wav_filename)[1]\n",
    "    save_png(\"%i_%s_wav_fft\" % (plot, os.path.splitext(rel_filename)[0]))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_wav_fft(\"sine_a.wav\", \"400Hz sine wave\", 1)\n",
    "plot_wav_fft(\"sine_b.wav\", \"3,000Hz sine wave\", 2)\n",
    "plot_wav_fft(\"sine_mix.wav\", \"Mixed sine wave\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"real\" music file looks a bit noisier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wav_fft(Path(GENRE_DIR) / 'disco' / 'disco.00000.wav', \"some sample song\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First classifier using FFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating FFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_fft(fn):\n",
    "    sample_rate, X = scipy.io.wavfile.read(fn)\n",
    "\n",
    "    fft_features = abs(scipy.fft(X)[:1000])\n",
    "    np.save(Path(fn).with_suffix('.fft'), fft_features)\n",
    "    \n",
    "for wav_fn in Path(GENRE_DIR).glob('**/*.wav'):\n",
    "    print('Converting %s ...' % str(wav_fn))\n",
    "    create_fft(wav_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fft(genre_list, base_dir=GENRE_DIR):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label, genre in enumerate(genre_list):\n",
    "        genre_dir = Path(base_dir) / genre\n",
    "        for fn in genre_dir.glob(\"*.fft.npy\"):\n",
    "            fft_features = np.load(fn)\n",
    "\n",
    "            X.append(fft_features[:1000])\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the FFT-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "\n",
    "def create_model():\n",
    "    return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def train_model(clf_factory, X, Y, name, plot=False):\n",
    "    labels = np.unique(Y)\n",
    "\n",
    "    cv = ShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    scores = []\n",
    "    pr_scores = defaultdict(list)\n",
    "    precisions, recalls, thresholds = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    roc_scores = defaultdict(list)\n",
    "    tprs = defaultdict(list)\n",
    "    fprs = defaultdict(list)\n",
    "\n",
    "    clfs = []  # just to later get the median\n",
    "\n",
    "    cms = []\n",
    "\n",
    "    for train, test in cv.split(X, Y):\n",
    "        X_train, y_train = X[train], Y[train]\n",
    "        X_test, y_test = X[test], Y[test]\n",
    "\n",
    "        clf = clf_factory()\n",
    "        clf.fit(X_train, y_train)\n",
    "        clfs.append(clf)\n",
    "\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "        scores.append(test_score)\n",
    "\n",
    "        train_errors.append(1 - train_score)\n",
    "        test_errors.append(1 - test_score)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cms.append(cm)\n",
    "\n",
    "        for label in labels:\n",
    "            y_label_test = np.asarray(y_test == label, dtype=int)\n",
    "            proba = clf.predict_proba(X_test)\n",
    "            proba_label = proba[:, label]\n",
    "\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(\n",
    "                y_label_test, proba_label)\n",
    "            pr_scores[label].append(auc(recall, precision))\n",
    "            precisions[label].append(precision)\n",
    "            recalls[label].append(recall)\n",
    "            thresholds[label].append(pr_thresholds)\n",
    "\n",
    "            fpr, tpr, roc_thresholds = roc_curve(y_label_test, proba_label)\n",
    "            roc_scores[label].append(auc(fpr, tpr))\n",
    "            tprs[label].append(tpr)\n",
    "            fprs[label].append(fpr)\n",
    "\n",
    "    if plot:\n",
    "        for idx, label in enumerate(labels):\n",
    "            print(\"Plotting %s\" % GENRES[label])\n",
    "            scores_to_sort = roc_scores[label]\n",
    "            median = np.argsort(scores_to_sort)[len(scores_to_sort) // 2]\n",
    "\n",
    "            desc = \"%s %s\" % (name, GENRES[label])\n",
    "            plot_pr(pr_scores[label][median], desc, precisions[label][median],\n",
    "                    recalls[label][median], label='%s vs rest' % GENRES[label], plot_nr=plot+idx)\n",
    "            plot_roc(roc_scores[label][median], desc, tprs[label][median],\n",
    "                     fprs[label][median], label='%s vs rest' % GENRES[label], plot_nr=plot+len(labels)+idx)\n",
    "\n",
    "    all_pr_scores = np.asarray(list(pr_scores.values())).flatten()\n",
    "    #import pdb;pdb.set_trace()\n",
    "    summary = (np.mean(scores), np.std(scores),\n",
    "               np.mean(all_pr_scores), np.std(all_pr_scores))\n",
    "    print(\"%.3f\\t%.3f\\t%.3f\\t%.3f\\t\" % summary)\n",
    "\n",
    "    return np.mean(train_errors), np.mean(test_errors), np.asarray(cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, genre_list, name, title, plot_nr=None):\n",
    "    plt.figure(num=None, figsize=(5, 4), dpi=DPI)\n",
    "    plt.matshow(cm, fignum=False, cmap='Blues', vmin=0, vmax=1.0)\n",
    "    ax = plt.axes()\n",
    "    ax.set_xticks(range(len(genre_list)))\n",
    "    ax.set_xticklabels(genre_list)\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.set_yticks(range(len(genre_list)))\n",
    "    ax.set_yticklabels(genre_list)\n",
    "    ax.tick_params(axis='both', which='both', bottom='off',  left='off')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.ylabel('True class')\n",
    "    if plot_nr is not None:\n",
    "        save_png('%i_confusion_%s' % (plot_nr, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y = read_fft(GENRES)\n",
    "\n",
    "train_avg, test_avg, cms = train_model(create_model, X, Y, \"Log Reg FFT\", plot=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm_avg = np.mean(cms, axis=0)\n",
    "cm_norm = cm_avg / np.sum(cm_avg, axis=0)\n",
    "\n",
    "plot_confusion_matrix(cm_norm, GENRES, \"fft\", \"Confusion matrix of an FFT based classifier\", 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving classification performance with Mel Frequency Cepstral Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "\n",
    "fn = Path(GENRE_DIR) / 'jazz' / 'jazz.00000.wav'\n",
    "sample_rate, X = scipy.io.wavfile.read(fn)\n",
    "ceps = mfcc(X)\n",
    "print(ceps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ceps = len(ceps)\n",
    "np.mean(ceps[int(num_ceps*0.1):int(num_ceps*0.9)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(ceps[int(num_ceps*0.1):int(num_ceps*0.9)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_ceps(fn):\n",
    "    sample_rate, X = scipy.io.wavfile.read(fn)\n",
    "    np.save(Path(fn).with_suffix('.ceps'), mfcc(X))\n",
    "\n",
    "for wav_fn in Path(GENRE_DIR).glob('**/*.wav'):\n",
    "    print('Converting %s ...' % str(wav_fn))\n",
    "    create_ceps(wav_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ceps(genre_list, base_dir=GENRE_DIR):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label, genre in enumerate(genre_list):\n",
    "        genre_dir = Path(base_dir) / genre\n",
    "        for fn in genre_dir.glob(\"*.ceps.npy\"):\n",
    "            ceps = np.load(fn)\n",
    "            num_ceps = len(ceps)\n",
    "            X.append(np.mean(ceps[int(num_ceps / 10):int(num_ceps * 9 / 10)], axis=0))\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the MFCC-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y = read_ceps(GENRES)\n",
    "\n",
    "train_avg, test_avg, cms = train_model(create_model, X, Y, \"Log Reg CEPS\", plot=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm_avg = np.mean(cms, axis=0)\n",
    "cm_norm = cm_avg / np.sum(cm_avg, axis=0)\n",
    "\n",
    "plot_confusion_matrix(cm_norm, GENRES, \"ceps\",\"Confusion matrix of a CEPS based classifier\", 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Fourier transform is a filter, which defines our features, we can also use CNN for the same kind of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_epochs = 50\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "step = 32\n",
    "dropout_rate = 0.2\n",
    "\n",
    "signal_size = 1000\n",
    "signal_shape = [signal_size,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 600 songs, we don't have enough data for a neural network. But we don't need to train the network wth just 1000 samples, we can split each of them to \"generate\" more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav(genre_list, multiplicity=1, base_dir=GENRE_DIR):\n",
    "    X = []\n",
    "    y = []\n",
    "    for label, genre in enumerate(genre_list):\n",
    "        genre_dir = Path(base_dir) / genre\n",
    "        for fn in genre_dir.glob(\"*.wav\"):\n",
    "            sample_rate, new_X = scipy.io.wavfile.read(fn)\n",
    "            for i in range(multiplicity):\n",
    "                X.append(new_X[i*signal_size:(i+1)*signal_size])\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X).reshape((-1, signal_size, 1)), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will get 20 excerpts from each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = read_wav(GENRES, 20)\n",
    "classes = len(GENRES)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=(1. / 6.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify our CNN class a little bit to use 1D convolution, and we also customize the pool size, as we expect it to require to be bigger for our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(\n",
    "            self,\n",
    "            signal_shape=[1000,1],\n",
    "            dim_W1=64,\n",
    "            dim_W2=32,\n",
    "            dim_W3=16,\n",
    "            classes=6,\n",
    "            kernel_size=5,\n",
    "            pool_size=16\n",
    "            ):\n",
    "\n",
    "        self.signal_shape = signal_shape\n",
    "\n",
    "        self.dim_W1 = dim_W1\n",
    "        self.dim_W2 = dim_W2\n",
    "        self.dim_W3 = dim_W3\n",
    "        self.classes = classes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def build_model(self):\n",
    "        image = tf.placeholder(tf.float32, [None]+self.signal_shape, name=\"signal\")\n",
    "        Y = tf.placeholder(tf.int64, [None], name=\"label\")\n",
    "        training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        probabilities = self.discriminate(image, training)\n",
    "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=probabilities))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(probabilities, axis=1), Y), tf.float32), name=\"accuracy\")\n",
    "\n",
    "        return image, Y, cost, accuracy, probabilities, training\n",
    "\n",
    "    def create_conv1d(self, input, filters, kernel_size, name):\n",
    "        layer = tf.layers.conv1d(\n",
    "                    inputs=input,\n",
    "                    filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation=tf.nn.leaky_relu,\n",
    "                    name=\"Conv1d_\" + name,\n",
    "                    padding=\"same\")\n",
    "        return layer\n",
    "    \n",
    "    def create_maxpool(self, input, name):\n",
    "        layer = tf.layers.max_pooling1d(\n",
    "                    inputs=input,\n",
    "                    pool_size=[self.pool_size],\n",
    "                    strides=self.pool_size,\n",
    "                    name=\"MaxPool_\" + name)\n",
    "        return layer\n",
    "\n",
    "    def create_dropout(self, input, name, is_training):\n",
    "        layer = tf.layers.dropout(\n",
    "                    inputs=input,\n",
    "                    rate=dropout_rate,\n",
    "                    name=\"DropOut_\" + name,\n",
    "                    training=is_training)\n",
    "        return layer\n",
    "\n",
    "    def create_dense(self, input, units, name):\n",
    "        layer = tf.layers.dense(\n",
    "                inputs=input,\n",
    "                units=units,\n",
    "                name=\"Dense\" + name,\n",
    "                )\n",
    "        layer = tf.layers.batch_normalization(\n",
    "                inputs=layer,\n",
    "                momentum=0,\n",
    "                epsilon=1e-8,\n",
    "                training=True,\n",
    "                name=\"BatchNorm_\" + name,\n",
    "        )\n",
    "        layer = tf.nn.leaky_relu(layer, name=\"LeakyRELU_\" + name)\n",
    "        return layer\n",
    "\n",
    "    def discriminate(self, signal, training):\n",
    "\n",
    "        h1 = self.create_conv1d(signal, self.dim_W3, self.kernel_size, \"Layer1\")\n",
    "        h1 = self.create_maxpool(h1, \"Layer1\")\n",
    "\n",
    "        h2 = self.create_conv1d(h1, self.dim_W2, self.kernel_size, \"Layer2\")\n",
    "        h2 = self.create_maxpool(h2, \"Layer2\")\n",
    "        h2 = tf.reshape(h2, (-1, self.dim_W2 * h2.shape[1]))\n",
    "\n",
    "        h3 = self.create_dense(h2, self.dim_W1, \"Layer3\")\n",
    "        h3 = self.create_dropout(h3, \"Layer3\", training)\n",
    "        \n",
    "        h4 = self.create_dense(h3, self.classes, \"Layer4\")\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a BaseEstimator subclass to use GridSearchCV to find the optimal set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self,\n",
    "            signal_shape=[1000,1],\n",
    "            dim_W1=64,\n",
    "            dim_W2=32,\n",
    "            dim_W3=16,\n",
    "            classes=6,\n",
    "            kernel_size=5,\n",
    "            pool_size=16):\n",
    "        self.signal_shape=signal_shape\n",
    "        self.dim_W1=dim_W1\n",
    "        self.dim_W2=dim_W2\n",
    "        self.dim_W3=dim_W3\n",
    "        self.classes=classes\n",
    "        self.kernel_size=kernel_size\n",
    "        self.pool_size=pool_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        print(\"Fitting (W1=%i) (W2=%i) (W3=%i) (kernel=%i) (pool=%i)\"\n",
    "              % (self.dim_W1, self.dim_W2, self.dim_W3, self.kernel_size, self.pool_size))\n",
    "        \n",
    "        cnn_model = CNN(\n",
    "                signal_shape=self.signal_shape,\n",
    "                dim_W1=self.dim_W1,\n",
    "                dim_W2=self.dim_W2,\n",
    "                dim_W3=self.dim_W3,\n",
    "                classes=self.classes,\n",
    "                kernel_size=self.kernel_size,\n",
    "                pool_size=self.pool_size\n",
    "                )\n",
    "\n",
    "        signal_tf, Y_tf, cost_tf, accuracy_tf, output_tf, training_tf = cnn_model.build_model()\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate, beta1=0.5).minimize(cost_tf)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(n_epochs):\n",
    "                permut = np.random.permutation(len(X_train))\n",
    "                for j in range(0, len(X_train), batch_size):\n",
    "                    batch = permut[j:j+batch_size]\n",
    "                    Xs = X_train[batch]\n",
    "                    Ys = Y_train[batch]\n",
    "\n",
    "                    sess.run(train_step,\n",
    "                            feed_dict={\n",
    "                                training_tf: True,\n",
    "                                Y_tf: Ys,\n",
    "                                signal_tf: Xs\n",
    "                                })\n",
    "            saver.save(sess, './classifier')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        tf.reset_default_graph()\n",
    "        new_saver = tf.train.import_meta_graph(\"classifier.meta\")  \n",
    "        with tf.Session() as sess:  \n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "            graph = tf.get_default_graph()\n",
    "            training_tf = graph.get_tensor_by_name('is_training:0')\n",
    "            signal_tf = graph.get_tensor_by_name('signal:0')\n",
    "            output_tf = graph.get_tensor_by_name('LeakyRELU_Layer4/Maximum:0')\n",
    "    \n",
    "            predict = sess.run(output_tf,\n",
    "                            feed_dict={\n",
    "                                training_tf: False,\n",
    "                                signal_tf: X\n",
    "                                })\n",
    "            return np.argmax(predict, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a grid of parameters to explore. Be aware that this will take a lot of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "param_grid = {\n",
    "    \"dim_W1\": [4, 8, 16],\n",
    "    \"dim_W2\": [4, 8, 16],\n",
    "    \"dim_W3\": [4, 8, 16],\n",
    "    \"kernel_size\":[7, 11, 15],\n",
    "    \"pool_size\":[8, 12, 16],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(Classifier(), param_grid, scoring=make_scorer(accuracy_score), cv=6)\n",
    "\n",
    "cv.fit(X, Y)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use these best parameters and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(**cv.best_params_)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_predict = clf.predict(X_train)\n",
    "Y_test_predict = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_train, Y_train_predict)\n",
    "plot_confusion_matrix(cm / np.sum(cm, axis=0), GENRES, \"CNN\",\"Confusion matrix of a CNN based classifier (train)\", 20)\n",
    "cm = confusion_matrix(Y_test, Y_test_predict)\n",
    "plot_confusion_matrix(cm / np.sum(cm, axis=0), GENRES, \"CNN\",\"Confusion matrix of a CNN based classifier (test)\", 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

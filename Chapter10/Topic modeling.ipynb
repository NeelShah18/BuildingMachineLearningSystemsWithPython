{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "We start with importing `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You cannot run this example only from within the notebook. You must first download the data on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, matutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the usual imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "\n",
    "# Check that data exists\n",
    "if not path.exists('./data/ap/ap.dat'):\n",
    "    print('Error: Expected data to be present at data/ap/')\n",
    "    print('Please cd into ./data & run ./download_ap.sh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate 100 topics as in the book, but you can changes this setting here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpora.BleiCorpus('./data/ap/ap.dat', './data/ap/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=NUM_TOPICS, id2word=corpus.id2word, alpha=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_used = [len(model[doc]) for doc in corpus]\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(num_topics_used, np.arange(42))\n",
    "ax.set_ylabel('Nr of documents')\n",
    "ax.set_xlabel('Nr of topics')\n",
    "fig.tight_layout()\n",
    "fig.savefig('Figure_04_01.png')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can do the same after changing the $\\alpha$ value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 1.0\n",
    "\n",
    "model1 = models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=NUM_TOPICS, id2word=corpus.id2word, alpha=ALPHA)\n",
    "num_topics_used1 = [len(model1[doc]) for doc in corpus]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist([num_topics_used, num_topics_used1], np.arange(42))\n",
    "ax.set_ylabel('Nr of documents')\n",
    "ax.set_xlabel('Nr of topics')\n",
    "\n",
    "# The coordinates below were fit by trial and error to look good\n",
    "ax.text(9, 223, r'default alpha')\n",
    "ax.text(26, 156, 'alpha=1.0')\n",
    "fig.tight_layout()\n",
    "fig.savefig('Figure_04_02.png')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the topic model\n",
    "\n",
    "We can explore the mathematical structure of the topics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus.docbyoffset(0)\n",
    "topics = model[doc]\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very informative, however. Another way to explore is to identify the most discussed topic, i.e., the one with the highest total weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = matutils.corpus2dense(model[corpus], num_terms=model.num_topics)\n",
    "weight = topics.sum(1)\n",
    "max_topic = weight.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 64 words for this topic.\n",
    "Without the argument, show_topic would return only 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = model.show_topic(max_topic, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualize the results is to build a _word cloud_. For this we use the `wordcloud` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(background_color='white', max_words=30, width=600, height=600)\n",
    "wc = wc.generate_from_frequencies(dict(words))\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, repeat the same exercise using alpha=1.0.\n",
    "\n",
    "You can edit the constant below to play around with this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.update(['from:', 'subject:', 'writes:', 'writes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add a little adaptor class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DirectText(corpora.textcorpus.TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        return self.input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "dataset = sklearn.datasets.load_mlcomp(\"20news-18828\", \"train\",\n",
    "                                       mlcomp_root='./data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess the data to split the data into words and remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otexts = dataset.data\n",
    "texts = dataset.data\n",
    "\n",
    "texts = [t.decode('utf-8', 'ignore') for t in texts]\n",
    "texts = [t.split() for t in texts]\n",
    "texts = [map(lambda w: w.lower(), t) for t in texts]\n",
    "texts = [filter(lambda s: not len(set(\"+-.?!()>@012345689\") & set(s)), t)\n",
    "         for t in texts]\n",
    "texts = [filter(lambda s: (len(s) > 3) and (s not in stopwords), t)\n",
    "         for t in texts]\n",
    "texts = [[english_stemmer.stem(w) for w in t] for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove words that are _too common_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "usage = defaultdict(int)\n",
    "for t in texts:\n",
    "    for w in set(t):\n",
    "        usage[w] += 1\n",
    "limit = len(texts) / 10\n",
    "too_common = [w for w in usage if usage[w] > limit]\n",
    "too_common = set(too_common)\n",
    "texts = [[w for w in t if w not in too_common] for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = DirectText(texts)\n",
    "dictionary = corpus.dictionary\n",
    "try:\n",
    "    dictionary['computer']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = models.ldamodel.LdaModel(\n",
    "    corpus, num_topics=100, id2word=dictionary.id2token)\n",
    "\n",
    "thetas = np.zeros((len(texts), 100))\n",
    "for i, c in enumerate(corpus):\n",
    "    for ti, v in model[c]:\n",
    "        thetas[i, ti] += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare all documents to each other **by the topics the contain**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "distances = distance.squareform(distance.pdist(thetas))\n",
    "large = distances.max() + 1\n",
    "for i in range(len(distances)):\n",
    "    distances[i, i] = large\n",
    "\n",
    "print(otexts[1])\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(otexts[distances[1].argmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modeling Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n",
    "\n",
    "Note that you **must have run the `wikitopics_create.py` script**. This will take a few hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "if not path.exists('wiki_lda.pkl'):\n",
    "    import sys\n",
    "    sys.stderr.write('''\\\n",
    "This script must be run after wikitopics_create.py!\n",
    "\n",
    "That script creates and saves the LDA model (this must onlly be done once).\n",
    "This script is responsible for the analysis.''')\n",
    "    \n",
    "# Load the preprocessed Wikipedia corpus (id2word and mm)\n",
    "id2word = gensim.corpora.Dictionary.load_from_text(\n",
    "    'data/wiki_en_output_wordids.txt.bz2')\n",
    "mm = gensim.corpora.MmCorpus('data/wiki_en_output_tfidf.mm')\n",
    "\n",
    "# Load the precomputed model\n",
    "model = gensim.models.ldamodel.LdaModel.load('wiki_lda.pkl')\n",
    "\n",
    "topics = np.load('topics.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of topics mentioned in each document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = (topics > 0).sum(axis=1)\n",
    "print('Mean number of topics mentioned: {0:.3}'.format(np.mean(lens)))\n",
    "print('Percentage of articles mentioning less than 10 topics: {0:.1%}'.format(np.mean(lens <= 10)))\n",
    "\n",
    "# Weights will be the total weight of each topic\n",
    "weights = topics.sum(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the most heavily used topic and plot it as a word cloud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.show_topic(weights.argmax(), 64)\n",
    "\n",
    "wc = WordCloud(background_color='white', max_words=30, width=600, height=600)\n",
    "wc = wc.generate_from_frequencies(dict(words))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_mention = np.mean(topics[:,weights.argmax()] > 0)\n",
    "print(\"The most mentioned topics is mentioned in {:.1%} of documents.\".format(fraction_mention))\n",
    "total_weight = np.mean(topics[:,weights.argmax()])\n",
    "print(\"It represents {:.1%} of the total number of words.\".format(total_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the **least** heavily used topic and plot it as a word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.show_topic(weights.argmin(), 64)\n",
    "\n",
    "wc = WordCloud(background_color='white', max_words=30, width=600, height=600)\n",
    "wc = wc.generate_from_frequencies(dict(words))\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.imshow(wc, interpolation=\"bilinear\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can measure how often this topic used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_mention = np.mean(topics[:,weights.argmin()] > 0)\n",
    "print(\"The least mentioned topics is mentioned in {:.1%} of documents.\".format(fraction_mention))\n",
    "total_weight = np.mean(topics[:,weights.argmin()])\n",
    "print(\"It represents {:.1%} of the total number of words.\".format(total_weight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
